{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes by - Kiran A Bendigeri\n",
    "Please Read 'Read me' file.\n",
    "\n",
    "Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''supervised learning is used whenever we want to predict a certain\n",
    "outcome from a given input, and we have examples of input/output pairs. We build a\n",
    "machine learning model from these input/output pairs, which comprise our training\n",
    "set.\n",
    "\n",
    "There are two major types of supervised machine learning problems, called classifca‐\n",
    "tion and regression.\n",
    "\n",
    " Classification is sometimes separated into binary classifcation,\n",
    "which is the special case of distinguishing between exactly two classes, and multiclass\n",
    "classifcation, which is classification between more than two classes.\n",
    "\n",
    "For regression tasks, the goal is to predict a continuous number, or a ﬂoating-point\n",
    "number in programming terms (or real number in mathematical terms).\n",
    "An easy way to distinguish between classification and regression tasks is to ask\n",
    "whether there is some kind of continuity in the output. If there is continuity between\n",
    "possible outcomes, then the problem is a regression problem.\n",
    "\n",
    "If a model is able to make accurate predictions on\n",
    "unseen data, we say it is able to generalize from the training set to the test set.\n",
    "\n",
    "Overfitting occurs when you fit a model too closely to the particularities of the training set and\n",
    "obtain a model that works well on the training set but is not able to generalize to new\n",
    "data.\n",
    "\n",
    "Choosing too simple a model is called underftting.'''\n",
    "\n",
    "'''Wisconsin Breast Cancer dataset (cancer,for short), which records clinical measurements of breast cancer tumors. Each tumor\n",
    "is labeled as “benign” (for harmless tumors) or “malignant” (for cancerous tumors),\n",
    "and the task is to learn to predict whether a tumor is malignant based on the measurements of the tissue.\n",
    "The data can be loaded using the load_breast_cancer function from scikit-learn:'''\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "import numpy as np\n",
    "import mglearn\n",
    "cancer = load_breast_cancer()\n",
    "print(\"cancer.keys(): \\n{}\".format(cancer.keys()))\n",
    "print(\"Shape of cancer data: {}\".format(cancer.data.shape))\n",
    "print(\"Sample counts per class:\\n{}\".format(\n",
    "        {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))\n",
    "print(\"Feature names:\\n{}\".format(cancer.feature_names))\n",
    "\n",
    "'''the Boston Housing dataset.\n",
    "The task associated with this dataset is to predict the median value of homes in sev‐\n",
    "eral Boston neighborhoods in the 1970s, using information such as crime rate, prox‐\n",
    "imity to the Charles River, highway accessibility, and so on. The dataset contains 506\n",
    "data points, described by 13 features:'''\n",
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(\"Data shape: {}\".format(boston.data.shape))\n",
    "\n",
    "'''The k-NN algorithm is arguably the simplest machine learning algorithm. Building\n",
    "the model consists only of storing the training dataset. To make a prediction for a\n",
    "new data point, the algorithm finds the closest data points in the training dataset—its\n",
    "“nearest neighbors.\n",
    "\n",
    "Instead of considering only the closest neighbor, we can also consider an arbitrary\n",
    "number, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\n",
    "comes from. When considering more than one neighbor, we use voting to assign a\n",
    "label. This means that for each test point, we count how many neighbors belong to\n",
    "class 0 and how many neighbors belong to class 1. We then assign the class that is\n",
    "more frequent: in other words, the majority class among the k-nearest neighbors.\n",
    "\n",
    " we split our data into a training and a test set so we can evaluate general‐\n",
    "ization performance'''\n",
    "mglearn.plots.plot_knn_classification(n_neighbors=3)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"Test set predictions: {}\".format(clf.predict(X_test)))\n",
    "print(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))\n",
    "\n",
    "'''The following code produces the visualizations of the decision boundaries for one,\n",
    "three, and nine neighbors'''\n",
    "import matplotlib.pyplot as plt\n",
    "fig, axes = plt.subplots(1, 3, figsize=(10, 3))\n",
    "for n_neighbors, ax in zip([1, 3, 9], axes):\n",
    "    # the fit method returns the object self, so we can instantiate\n",
    "    # and fit in one line\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n",
    "    ax.set_xlabel(\"feature 0\")\n",
    "    ax.set_ylabel(\"feature 1\")\n",
    "axes[0].legend(loc=3)\n",
    "\n",
    "''' Breast Cancer dataset.'''\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "cancer.data, cancer.target, stratify=cancer.target, random_state=66)\n",
    "training_accuracy = []\n",
    "test_accuracy = []\n",
    "# try n_neighbors from 1 to 10\n",
    "neighbors_settings = range(1, 11)\n",
    "for n_neighbors in neighbors_settings:\n",
    "    # build the model\n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # record training set accuracy\n",
    "    training_accuracy.append(clf.score(X_train, y_train))\n",
    "    # record generalization accuracy\n",
    "    test_accuracy.append(clf.score(X_test, y_test))\n",
    "plt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\n",
    "plt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "'''k-neighbors regression\n",
    "'''\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "X, y = mglearn.datasets.make_wave(n_samples=40)\n",
    "# split the wave dataset into a training and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "# instantiate the model and set the number of neighbors to consider to 3\n",
    "reg = KNeighborsRegressor(n_neighbors=3)\n",
    "# fit the model using the training data and training targets\n",
    "reg.fit(X_train, y_train)\n",
    "print(\"Test set predictions:\\n{}\".format(reg.predict(X_test)))\n",
    "print(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "# create 1,000 data points, evenly spaced between -3 and 3\n",
    "line = np.linspace(-3, 3, 1000).reshape(-1, 1)\n",
    "for n_neighbors, ax in zip([1, 3, 9], axes):\n",
    "    # make predictions using 1, 3, or 9 neighbors\n",
    "    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n",
    "    reg.fit(X_train, y_train)\n",
    "    ax.plot(line, reg.predict(line))\n",
    "    ax.plot(X_train, y_train, '^', c=mglearn.cm2(0), markersize=8)\n",
    "    ax.plot(X_test, y_test, 'v', c=mglearn.cm2(1), markersize=8)\n",
    "    ax.set_title(\n",
    "    \"{} neighbor(s)\\n train score: {:.2f} test score: {:.2f}\".format(\n",
    "    n_neighbors, reg.score(X_train, y_train),\n",
    "    reg.score(X_test, y_test)))\n",
    "    ax.set_xlabel(\"Feature\")\n",
    "    ax.set_ylabel(\"Target\")\n",
    "axes[0].legend([\"Model predictions\", \"Training data/target\",\n",
    "    \"Test data/target\"], loc=\"best\")\n",
    "\n",
    "'''Linear Models\n",
    "Linear models for regression\n",
    "For regression, the general prediction formula for a linear model looks as follows:\n",
    "ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\n",
    "Here, x[0] to x[p] denotes the features (in this example, the number of features is p)\n",
    "of a single data point, w and b are parameters of the model that are learned, and ŷ is\n",
    "the prediction the model makes. '''\n",
    "mglearn.plots.plot_linear_regression_wave()\n",
    "'''Linear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\n",
    "ear method for regression. Linear regression finds the parameters w and b that mini‐\n",
    "mize the mean squared error between predictions and the true regression targets, y,\n",
    "on the training set.'''\n",
    "from sklearn.linear_model import LinearRegression\n",
    "X, y = mglearn.datasets.make_wave(n_samples=60)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "print(\"lr.coef_: {}\".format(lr.coef_))\n",
    "print(\"lr.intercept_: {}\".format(lr.intercept_))\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n",
    "'''An R2 of around 0.66 is not very good, but we can see that the scores on the training\n",
    "and test sets are very close together. This means we are likely underfitting, not over‐\n",
    "fitting. For this one-dimensional dataset, there is little danger of overfitting, as the\n",
    "model is very simple (or restricted).'''\n",
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n",
    "'''This discrepancy between performance on the training set and the test set is a clear\n",
    "sign of overfitting, and therefore we should try to find a model that allows us to con‐\n",
    "trol complexity. One of the most commonly used alternatives to standard linear\n",
    "regression is ridge regression\n",
    " \n",
    "In ridge regression,\n",
    "though, the coefficients (w) are chosen not only so that they predict well on the train‐\n",
    "ing data, but also to fit an additional constraint. We also want the magnitude of coef‐\n",
    "ficients to be as small as possible; in other words, all entries of w should be close to\n",
    "zero. Intuitively, this means each feature should have as little effect on the outcome as\n",
    "possible (which translates to having a small slope), while still predicting well. This\n",
    "constraint is an example of what is called regularization. Regularization means explic‐\n",
    "itly restricting a model to avoid overfitting. The particular kind used by ridge regres‐\n",
    "sion is known as L2 regularization. '''\n",
    "from sklearn.linear_model import Ridge\n",
    "ridge = Ridge().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\n",
    "'''the training set score of Ridge is lower than for LinearRegression,\n",
    "while the test set score is higher. This is consistent with our expectation. With linear\n",
    "regression, we were overfitting our data. Ridge is a more restricted model, so we are\n",
    "less likely to overfit. A less complex model means worse performance on the training\n",
    "set, but better generalization. As we are only interested in generalization perfor‐\n",
    "mance, we should choose the Ridge model over the LinearRegression model.'''\n",
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))\n",
    "'''Decreasing alpha allows the coefficients to be less restricted. For very small values of alpha, coefficients are barely restricted at all,\n",
    "and we end up with a model that resembles LinearRegression:'''\n",
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))\n",
    "\n",
    "plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\n",
    "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
    "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
    "plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.hlines(0, 0, len(lr.coef_))\n",
    "plt.ylim(-25, 25)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "'''An alternative to Ridge for regularizing linear regression is Lasso. As with ridge\n",
    "regression, using the lasso also restricts coefficients to be close to zero, but in a\n",
    "slightly different way, called L1 regularization.8 The consequence of L1 regularization\n",
    "is that when using the lasso, some coefficients are exactly zero. This means some fea‐\n",
    "tures are entirely ignored by the model. This can be seen as a form of automatic fea‐\n",
    "ture selection. Having some coefficients be exactly zero often makes a model easier to\n",
    "interpret, and can reveal the most important features of your model.\n",
    "Let’s apply the lasso to the extended Boston Housing dataset:'''\n",
    "from sklearn.linear_model import Lasso\n",
    "lasso = Lasso().fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\n",
    "\n",
    "# we increase the default setting of \"max_iter\",\n",
    "# otherwise the model would warn us that we should increase max_iter.\n",
    "lasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))\n",
    "\n",
    "'''A lower alpha allowed us to fit a more complex model, which worked better on the\n",
    "training and test data.'''\n",
    "lasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\n",
    "print(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))\n",
    "plt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\n",
    "plt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\n",
    "plt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\n",
    "plt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\n",
    "plt.legend(ncol=2, loc=(0, 1.05))\n",
    "plt.ylim(-25, 25)\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.show()\n",
    "\n",
    "'''Linear models are also extensively used for classification. Let’s look at binary classifi‐\n",
    "cation first. In this case, a prediction is made using the following formula:\n",
    "ŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\n",
    "\n",
    "The formula looks very similar to the one for linear regression, but instead of just\n",
    "returning the weighted sum of the features, we threshold the predicted value at zero.\n",
    "If the function is smaller than zero, we predict the class –1; if it is larger than zero, we\n",
    "predict the class +1. This prediction rule is common to all linear models for classifica‐\n",
    "tion.\n",
    "\n",
    "There are many algorithms for learning linear models. These algorithms all differ in\n",
    "the following two ways:\n",
    "• The way in which they measure how well a particular combination of coefficients\n",
    "and intercept fits the training data\n",
    "• If and what kind of regularization they use\n",
    "\n",
    "The two most common linear classification algorithms are logistic regression, imple‐\n",
    "mented in linear_model.LogisticRegression, and linear support vector machines\n",
    "(linear SVMs), implemented in svm.'''\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "X, y = mglearn.datasets.make_forge()\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 3))\n",
    "for model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n",
    "    clf = model.fit(X, y)\n",
    "    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n",
    "                                    ax=ax, alpha=.7)\n",
    "    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n",
    "    ax.set_title(\"{}\".format(clf.__class__.__name__))\n",
    "    ax.set_xlabel(\"Feature 0\")\n",
    "    ax.set_ylabel(\"Feature 1\")\n",
    "axes[0].legend()\n",
    "\n",
    "mglearn.plots.plot_linear_svc_regularization()\n",
    "\n",
    "from sklearn.datasets import make_blobs\n",
    "X, y = make_blobs(random_state=42)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "plt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\n",
    "plt.show()\n",
    "\n",
    "'''The main parameter of linear models is the regularization parameter, called alpha in\n",
    "the regression models and C in LinearSVC and LogisticRegression. Large values for\n",
    "alpha or small values for C mean simple models.\n",
    " Usually C and alpha are searched for\n",
    "on a logarithmic scale. The other decision you have to make is whether you want to\n",
    "use L1 regularization or L2 regularization. If you assume that only a few of your fea‐\n",
    "tures are actually important, you should use L1. Otherwise, you should default to L2.\n",
    "L1 can also be useful if interpretability of the model is important. As L1 will use only\n",
    "a few features, it is easier to explain which features are important to the model, and\n",
    "what the effects of these features are.\n",
    "Linear models are very fast to train, and also fast to predict. They scale to very large\n",
    "datasets and work well with sparse data. If your data consists of hundreds of thou‐\n",
    "sands or millions of samples, you might want to investigate using the solver='sag'\n",
    "option in LogisticRegression and Ridge, which can be faster than the default on\n",
    "large datasets. Other options are the SGDClassifier class and the SGDRegressor\n",
    "class, which implement even more scalable versions of the linear models described\n",
    "here.\n",
    "Another strength of linear models is that they make it relatively easy to understand\n",
    "how a prediction is made, using the formulas we saw earlier for regression and classi‐\n",
    "fication. Unfortunately, it is often not entirely clear why coefficients are the way they\n",
    "are. This is particularly true if your dataset has highly correlated features; in these\n",
    "cases, the coefficients might be hard to interpret.\n",
    "Linear models often perform well when the number of features is large compared to\n",
    "the number of samples. They are also often used on very large datasets, simply\n",
    "because it’s not feasible to train other models. However, in lower-dimensional spaces,\n",
    "other models might yield better generalization performance. We will look at some\n",
    "examples in which linear models fail in “Kernelized Support Vector Machines” '''\n",
    "\n",
    "'''Naive Bayes classifiers are a family of classifiers that are quite similar to the linear\n",
    "models \n",
    "they tend to be even faster in training. The price paid for this efficiency is that naive Bayes models often provide\n",
    "generalization performance that is slightly worse than that of linear classifiers like\n",
    "LogisticRegression and LinearSVC.\n",
    "GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\n",
    "any continuous data, while BernoulliNB assumes binary data and MultinomialNB\n",
    "assumes count data (that is, that each feature represents an integer count of some‐\n",
    "thing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\n",
    "are mostly used in text data classification.\n",
    "\n",
    "MultinomialNB and BernoulliNB have a single parameter, alpha, which controls\n",
    "model complexity. The way alpha works is that the algorithm adds to the data alpha\n",
    "many virtual data points that have positive values for all the features. This results in a\n",
    "“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\n",
    "complex models. The algorithm’s performance is relatively robust to the setting of\n",
    "alpha, meaning that setting alpha is not critical for good performance. However,\n",
    "tuning it usually improves accuracy somewhat.\n",
    "GaussianNB is mostly used on very high-dimensional data, while the other two var‐\n",
    "iants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\n",
    "usually performs better than BinaryNB, particularly on datasets with a relatively large\n",
    "number of nonzero features (i.e., large documents).\n",
    "The naive Bayes models share many of the strengths and weaknesses of the linear\n",
    "models. They are very fast to train and to predict, and the training procedure is easy\n",
    "to understand. The models work very well with high-dimensional sparse data and are\n",
    "relatively robust to the parameters. Naive Bayes models are great baseline models and\n",
    "are often used on very large datasets, where training even a linear model might take\n",
    "too long.\n",
    "'''\n",
    "\n",
    "'''Decision Trees\n",
    "Decision trees are widely used models for classification and regression tasks. Essen‐\n",
    "tially, they learn a hierarchy of if/else questions, leading to a decision.'''\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "cancer = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "cancer.data, cancer.target, stratify=cancer.target, random_state=42)\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "'''If we don’t restrict the depth of a decision tree, the tree can become arbitrarily deep\n",
    "and complex. Unpruned trees are therefore prone to overfitting and not generalizing\n",
    "well to new data. Now let’s apply pre-pruning to the tree, which will stop developing\n",
    "the tree before we perfectly fit to the training data. One option is to stop building the\n",
    "tree after a certain depth has been reached. Here we set max_depth=4, meaning only\n",
    "four consecutive questions can be asked'''\n",
    "tree = DecisionTreeClassifier(max_depth=4, random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\n",
    "'''We can visualize the tree using the export_graphviz function from the tree module.\n",
    "This writes a file in the .dot file format, which is a text file format for storing graphs.'''\n",
    "from sklearn.tree import export_graphviz\n",
    "export_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\n",
    "feature_names=cancer.feature_names, impurity=False, filled=True)\n",
    "import graphviz\n",
    "with open(\"tree.dot\") as f:\n",
    "    dot_graph = f.read()\n",
    "graphviz.Source(dot_graph)\n",
    "\n",
    "''' a forecast for the years after 2000 using the historical data up to that\n",
    "point, with the date as our only feature. We will compare two simple models: a\n",
    "DecisionTreeRegressor and LinearRegression. We rescale the prices using a loga‐\n",
    "rithm, so that the relationship is relatively linear. This doesn’t make a difference for\n",
    "the DecisionTreeRegressor, but it makes a big difference for LinearRegression'''\n",
    "import pandas as pd\n",
    "ram_prices = pd.read_csv(\"data/ram_price.csv\")\n",
    "plt.semilogy(ram_prices.date, ram_prices.price)\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylabel(\"Price in $/Mbyte\")\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# use historical data to forecast prices after the year 2000\n",
    "data_train = ram_prices[ram_prices.date < 2000]\n",
    "data_test = ram_prices[ram_prices.date >= 2000]\n",
    "# predict prices based on date\n",
    "X_train = data_train.date[:, np.newaxis]\n",
    "# we use a log-transform to get a simpler relationship of data to target\n",
    "y_train = np.log(data_train.price)\n",
    "tree = DecisionTreeRegressor().fit(X_train, y_train)\n",
    "linear_reg = LinearRegression().fit(X_train, y_train)\n",
    "# predict on all data\n",
    "X_all = ram_prices.date[:, np.newaxis]\n",
    "pred_tree = tree.predict(X_all)\n",
    "pred_lr = linear_reg.predict(X_all)\n",
    "# undo log-transform\n",
    "price_tree = np.exp(pred_tree)\n",
    "price_lr = np.exp(pred_lr)\n",
    "plt.semilogy(data_train.date, data_train.price, label=\"Training data\")\n",
    "plt.semilogy(data_test.date, data_test.price, label=\"Test data\")\n",
    "plt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\n",
    "plt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "'''Decision trees have two advantages over many of the algorithms we’ve discussed so\n",
    "far: the resulting model can easily be visualized and understood by nonexperts (at\n",
    "least for smaller trees), and the algorithms are completely invariant to scaling of the\n",
    "data. As each feature is processed separately, and the possible splits of the data don’t\n",
    "depend on scaling, no preprocessing like normalization or standardization of features\n",
    "is needed for decision tree algorithms. In particular, decision trees work well when\n",
    "you have features that are on completely different scales, or a mix of binary and con‐\n",
    "tinuous features.\n",
    "The main downside of decision trees is that even with the use of pre-pruning, they\n",
    "tend to overfit and provide poor generalization performance. Therefore, in most\n",
    "applications, the ensemble methods we discuss next are usually used in place of a sin‐\n",
    "gle decision tree.'''\n",
    "\n",
    "'''Ensembles are methods that combine multiple machine learning models to create\n",
    "more powerful models. There are many models in the machine learning literature\n",
    "that belong to this category, but there are two ensemble models that have proven to\n",
    "be effective on a wide range of datasets for classification and regression, both of\n",
    "which use decision trees as their building blocks: random forests and gradient boos‐\n",
    "ted decision trees.'''\n",
    "\n",
    "'''main drawback of decision trees is that they tend to overfit the\n",
    "training data. Random forests are one way to address this problem. A random forest\n",
    "is essentially a collection of decision trees, where each tree is slightly different from\n",
    "the others. The idea behind random forests is that each tree might do a relatively\n",
    "good job of predicting, but will likely overfit on part of the data. If we build many\n",
    "trees, all of which work well and overfit in different ways, we can reduce the amount\n",
    "of overfitting by averaging their results. This reduction in overfitting, while retaining\n",
    "the predictive power of the trees, can be shown using rigorous mathematics.'''\n",
    "\n",
    "''' There are two ways in which the trees in a random\n",
    "forest are randomized: by selecting the data points used to build a tree and by select‐\n",
    "ing the features in each split test. \n",
    "To build a random forest model, you need to decide on the\n",
    "number of trees to build (the n_estimators parameter of RandomForestRegressor or\n",
    "RandomForestClassifier). Let’s say we want to build 10 trees. These trees will be\n",
    "built completely independently from each other, and the algorithm will make differ‐\n",
    "ent random choices for each tree to make sure the trees are distinct. To build a tree,\n",
    "we first take what is called a bootstrap sample of our data. That is, from our n_samples\n",
    "data points, we repeatedly draw an example randomly with replacement (meaning the\n",
    "same sample can be picked multiple times), n_samples times. This will create a data‐\n",
    "set that is as big as the original dataset, but some data points will be missing from it\n",
    "(approximately one third), and some will be repeated.'''\n",
    "'''Let’s apply a random forest consisting of five trees to the\n",
    "two_moons dataset we studied earlier:'''\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "random_state=42)\n",
    "forest = RandomForestClassifier(n_estimators=5, random_state=2)\n",
    "forest.fit(X_train, y_train)\n",
    "'''The trees that are built as part of the random forest are stored in the estimator_\n",
    "attribute. Let’s visualize the decision boundaries learned by each tree, together with\n",
    "their aggregate prediction as made by the forest (Figure 2-33):'''\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 10))\n",
    "for i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n",
    "    ax.set_title(\"Tree {}\".format(i))\n",
    "    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\n",
    "mglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],\n",
    "alpha=.4)\n",
    "axes[-1, -1].set_title(\"Random Forest\")\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "\n",
    "'''apply a random forest consisting of 100 trees on the Breast\n",
    "Cancer dataset:'''\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "cancer.data, cancer.target, random_state=0)\n",
    "forest = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "forest.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\n",
    "''' building random forests on large data‐\n",
    "sets might be somewhat time consuming, '''\n",
    "\n",
    "'''The gradient boosted regression tree is another ensemble method that combines mul‐\n",
    "tiple decision trees to create a more powerful model. Despite the “regression” in the\n",
    "name, these models can be used for regression and classification. In contrast to the\n",
    "random forest approach, gradient boosting works by building trees in a serial man‐\n",
    "ner, where each tree tries to correct the mistakes of the previous one. By default, there\n",
    "is no randomization in gradient boosted regression trees; instead, strong pre-pruning\n",
    "is used. Gradient boosted trees often use very shallow trees, of depth one to five,\n",
    "which makes the model smaller in terms of memory and makes predictions faster.\n",
    "The main idea behind gradient boosting is to combine many simple models (in this\n",
    "context known as weak learners), like shallow trees. Each tree can only provide good\n",
    "predictions on part of the data, and so more and more trees are added to iteratively\n",
    "improve performance.'''\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "cancer.data, cancer.target, random_state=0)\n",
    "gbrt = GradientBoostingClassifier(random_state=0)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\n",
    "\n",
    "'''As the training set accuracy is 100%, we are likely to be overfitting. To reduce overfit‐\n",
    "ting, we could either apply stronger pre-pruning by limiting the maximum depth or\n",
    "lower the learning rate:'''\n",
    "gbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\n",
    "\n",
    "gbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\n",
    "gbrt.fit(X_train, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\n",
    "\n",
    "'''Gradient boosted decision trees are among the\n",
    "most powerful and widely used models for supervised learning. Their main drawback\n",
    "is that they require careful tuning of the parameters and may take a long time to\n",
    "train. Similarly to other tree-based models, the algorithm works well without scaling\n",
    "and on a mixture of binary and continuous features. As with other tree-based models,\n",
    "it also often does not work well on high-dimensional sparse data.'''\n",
    "\n",
    "'''Kernelized support vector machines\n",
    "(often just referred to as SVMs) are an extension that allows for more complex mod‐\n",
    "els that are not defined simply by hyperplanes in the input space. While there are sup‐\n",
    "port vector machines for classification and regression, we will restrict ourselves to the\n",
    "classification case, as implemented in SVC. Similar concepts apply to support vector\n",
    "regression, as implemented in SVR.'''\n",
    "X, y = make_blobs(centers=4, random_state=8)\n",
    "y = y % 2\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "linear_svm = LinearSVC().fit(X, y)\n",
    "mglearn.plots.plot_2d_separator(linear_svm, X)\n",
    "mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "\n",
    "# add the squared first feature\n",
    "X_new = np.hstack([X, X[:, 1:] ** 2])\n",
    "from mpl_toolkits.mplot3d import Axes3D, axes3d\n",
    "figure = plt.figure()\n",
    "# visualize in 3D\n",
    "ax = Axes3D(figure, elev=-152, azim=-26)\n",
    "# plot first all the points with y == 0, then all with y == 1\n",
    "mask = y == 0\n",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n",
    "cmap=mglearn.cm2, s=60)\n",
    "ax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n",
    "cmap=mglearn.cm2, s=60)\n",
    "ax.set_xlabel(\"feature0\")\n",
    "ax.set_ylabel(\"feature1\")\n",
    "ax.set_zlabel(\"feature1 ** 2\")\n",
    "'''Kernelized support vector machines are powerful models and perform well on a vari‐\n",
    "ety of datasets. SVMs allow for complex decision boundaries, even if the data has only\n",
    "a few features. They work well on low-dimensional and high-dimensional data (i.e.,\n",
    "few and many features), but don’t scale very well with the number of samples. Run‐\n",
    "ning an SVM on data with up to 10,000 samples might work well, but working with\n",
    "datasets of size 100,000 or more can become challenging in terms of runtime and\n",
    "memory usage.\n",
    "Another downside of SVMs is that they require careful preprocessing of the data and\n",
    "tuning of the parameters. This is why, these days, most people instead use tree-based\n",
    "models such as random forests or gradient boosting (which require little or no pre‐\n",
    "processing) in many applications. Furthermore, SVM models are hard to inspect; it\n",
    "can be difficult to understand why a particular prediction was made, and it might be\n",
    "tricky to explain the model to a nonexpert.'''\n",
    "\n",
    "'''Neural Networks (Deep Learning)\n",
    "A family of algorithms known as neural networks has recently seen a revival under\n",
    "the name “deep learning.” While deep learning shows great promise in many machine\n",
    "learning applications, deep learning algorithms are often tailored very carefully to a\n",
    "specific use case. Here, we will only discuss some relatively simple methods, namely\n",
    "multilayer perceptrons for classification and regression, that can serve as a starting\n",
    "point for more involved deep learning methods. Multilayer perceptrons (MLPs) are\n",
    "also known as (vanilla) feed-forward neural networks, or sometimes just neural\n",
    "networks.'''\n",
    "'''Having large neural networks made up of many of these layers of computation is\n",
    "what inspired the term “deep learning'''\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=100, noise=0.25, random_state=3)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n",
    "random_state=42)\n",
    "mlp = MLPClassifier(algorithm='l-bfgs', random_state=0).fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "\n",
    "'''By default, the MLP uses 100 hidden nodes, which is quite a lot for this small dataset.\n",
    "We can reduce the number (which reduces the complexity of the model) and still get\n",
    "a good result (Figure 2-49):'''\n",
    "\n",
    "mlp = MLPClassifier(algorithm='l-bfgs', random_state=0, hidden_layer_sizes=[10])\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "\n",
    "# using two hidden layers, with 10 units each, now with tanh nonlinearity\n",
    "mlp = MLPClassifier(algorithm='l-bfgs', activation='tanh',\n",
    "random_state=0, hidden_layer_sizes=[10, 10])\n",
    "mlp.fit(X_train, y_train)\n",
    "mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\n",
    "mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\n",
    "plt.xlabel(\"Feature 0\")\n",
    "plt.ylabel(\"Feature 1\")\n",
    "\n",
    "# compute the mean value per feature on the training set\n",
    "mean_on_train = X_train.mean(axis=0)\n",
    "# compute the standard deviation of each feature on the training set\n",
    "std_on_train = X_train.std(axis=0)\n",
    "# subtract the mean, and scale by inverse standard deviation\n",
    "# afterward, mean=0 and std=1\n",
    "X_train_scaled = (X_train - mean_on_train) / std_on_train\n",
    "# use THE SAME transformation (using training mean and std) on the test set\n",
    "X_test_scaled = (X_test - mean_on_train) / std_on_train\n",
    "mlp = MLPClassifier(random_state=0)\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "print(\"Accuracy on training set: {:.3f}\".format(\n",
    "mlp.score(X_train_scaled, y_train)))\n",
    "print(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\n",
    "\n",
    "'''Neural networks have reemerged as state-of-the-art models in many applications of\n",
    "machine learning. One of their main advantages is that they are able to capture infor‐\n",
    "mation contained in large amounts of data and build incredibly complex models.\n",
    "Given enough computation time, data, and careful tuning of the parameters, neural\n",
    "networks often beat other machine learning algorithms (for classification and regres‐\n",
    "sion tasks).\n",
    "Supervised Machine Learning Algorithms | 117This brings us to the downsides. Neural networks—particularly the large and power‐\n",
    "ful ones—often take a long time to train. They also require careful preprocessing of\n",
    "the data, as we saw here. Similarly to SVMs, they work best with “homogeneous”\n",
    "data, where all the features have similar meanings. For data that has very different\n",
    "kinds of features, tree-based models might work better. Tuning neural network\n",
    "parameters is also an art unto itself. In our experiments, we barely scratched the sur‐\n",
    "face of possible ways to adjust neural network models and how to train them.'''\n",
    "\n",
    "'''Uncertainty Estimates from Classifers\n",
    "Another useful part of the scikit-learn interface that we haven’t talked about yet is\n",
    "the ability of classifiers to provide uncertainty estimates of predictions. Often, you are\n",
    "not only interested in which class a classifier predicts for a certain test point, but also\n",
    "how certain it is that this is the right class. In practice, different kinds of mistakes lead\n",
    "to very different outcomes in real-world applications. Imagine a medical application\n",
    "testing for cancer. Making a false positive prediction might lead to a patient undergo‐\n",
    "ing additional tests, while a false negative prediction might lead to a serious disease\n",
    "not being treated.'''\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
